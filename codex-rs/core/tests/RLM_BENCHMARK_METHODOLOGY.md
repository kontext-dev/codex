# RLM Benchmark Methodology

This document describes the methodology for benchmarking the RLM (Retrieval-augmented Language Model) system against baseline and code mode approaches for handling Gateway tool results.

## Overview

The benchmark compares three approaches for processing large tool results from MCP Gateway servers:

| Approach | Description | Token Strategy |
|----------|-------------|----------------|
| **Baseline** | EXECUTE_TOOL - Full results in context | All tokens in conversation |
| **Code Mode** | EXECUTE_CODE - Batched with fixed summarization | Summarized tokens only |
| **RLM Mode** | Corpus storage with bounded access | Evidence summaries + sub-LM decomposition |

## Simulated Workloads

Benchmarks use simulated tool results based on real Linear API response patterns:

```
┌──────────────────┬────────────┬───────────┬─────────────────────────────┐
│ Tool             │ Tokens     │ Latency   │ Description                 │
├──────────────────┼────────────┼───────────┼─────────────────────────────┤
│ list_projects    │ 2,000      │ 200ms     │ 15 projects with metadata   │
│ list_issues      │ 45,000     │ 12,900ms  │ 200 issues with full details│
│ list_users       │ 500        │ 87ms      │ 50 users with basic info    │
│ get_issue        │ 800        │ 150ms     │ Single issue with comments  │
│ list_documents   │ 8,000      │ 235ms     │ 100 documents with content  │
└──────────────────┴────────────┴───────────┴─────────────────────────────┘
```

### Why Simulation?

1. **Reproducibility**: Real API calls vary; simulated data ensures consistent comparisons
2. **No credentials required**: Benchmarks run in CI without Gateway authentication
3. **Controlled variables**: Isolates RLM overhead from network variability
4. **Scalability testing**: Can simulate arbitrarily large responses

## Benchmark Scenarios

### 1. Simple List (`simple_list`)

```
Query: "Get the count of issues and count of projects."
Tools: [list_issues, list_projects]
Pattern: 2 independent parallel calls
```

**Purpose**: Tests basic parallel execution. All approaches should handle this well.

### 2. Fan-Out (`fan_out`)

```
Query: "List all projects, then summarize the issues in each one."
Tools: [list_projects, list_issues × 3]
Pattern: 1 list + N detail calls requiring per-entity reasoning
```

**Purpose**: Stress test for context overflow. Baseline fails here; Code Mode loses detail; RLM uses sub-LM decomposition.

### 3. Cross-Entity (`cross_entity`)

```
Query: "Analyze team workload distribution and identify bottlenecks."
Tools: [list_users, list_projects, list_issues, list_documents]
Pattern: Multi-entity analysis requiring cross-referencing
```

**Purpose**: Tests reasoning across different data types. Code Mode's fixed summarization cannot cross-reference.

### 4. Detail Lookup (`detail_lookup`)

```
Query: "Get details on 5 specific issues."
Tools: [get_issue × 5]
Pattern: Multiple detail lookups
```

**Purpose**: Tests repeated small lookups. All approaches work but with different efficiency.

## Metrics

### Primary Metrics

| Metric | Description | Unit |
|--------|-------------|------|
| `context_tokens` | Tokens in LLM input context | count |
| `completion_tokens` | Tokens generated by LLM | count |
| `total_tokens` | context + completion | count |
| `latency_ms` | Total execution time | milliseconds |
| `quality_score` | Reasoning capability preserved | 0-100% |

### RLM-Specific Metrics

| Metric | Description | Unit |
|--------|-------------|------|
| `evidence_count` | Evidence items tracked | count |
| `chunk_count` | Corpus chunks created | count |
| `sub_lm_calls` | Sub-LM invocations | count |

### Cost Estimation

Based on GPT-4o-mini pricing (representative):
- Prompt: $0.15 / 1M tokens
- Completion: $0.60 / 1M tokens

```
cost = (context_tokens × 0.15 + completion_tokens × 0.60) / 1,000,000
```

## Mode Calculations

### Baseline Mode

```python
context_tokens = system_prompt(300) + sum(tool_result_tokens)
completion_tokens = tool_calls(50 × n) + final_response(200)
latency = sum(tool_latencies)  # Sequential
quality = 100 if context_tokens < 128000 else 0  # Context overflow
```

### Code Mode

```python
context_tokens = system_prompt(200) + sum(tool_result_tokens × 0.10)  # 10% summarization
completion_tokens = code_generation(100 + 30 × n)
latency = max(tool_latencies) + vm_overhead(50)  # Parallel
quality = scenario_dependent(25-100)  # Fixed summarization limits
```

### RLM Mode

```python
context_tokens = system_prompt(500) + evidence_summaries(100 × n)
completion_tokens = root_response(200) + sub_lm_calls(500 × depth)
latency = max(tool_latencies) + rlm_overhead(1-15ms)  # Parallel + routing
quality = 100  # Bounded decomposition preserves reasoning
```

## Quality Scoring

Quality reflects the approach's ability to perform the required reasoning:

| Score | Meaning |
|-------|---------|
| 100% | Full reasoning capability preserved |
| 50% | Partial - some analysis possible |
| 25% | Minimal - only basic aggregation |
| 0% | Failure - context overflow or error |

### Scenario Quality Matrix

| Scenario | Baseline | Code Mode | RLM |
|----------|----------|-----------|-----|
| simple_list | 100% | 100% | 100% |
| fan_out | 0%* | 40% | 100% |
| cross_entity | 100% | 25% | 100% |
| detail_lookup | 100% | 50% | 100% |

*Baseline fails on fan_out due to context overflow (137K tokens > 128K limit)

## Running Benchmarks

### Three-Way Comparison

```bash
cargo test -p codex-core --test rlm_gateway_benchmark \
  benchmark_three_way_comparison -- --nocapture
```

### Infrastructure Overhead

```bash
cargo test -p codex-core --test rlm_gateway_benchmark \
  benchmark_rlm_infrastructure_overhead -- --nocapture
```

### Evidence Summary Performance

```bash
cargo test -p codex-core --test rlm_gateway_benchmark \
  benchmark_evidence_summary_generation -- --nocapture
```

### All Benchmarks

```bash
cargo test -p codex-core --test rlm_gateway_benchmark -- --nocapture
```

## Interpreting Results

### Output Format

```
════════════════════════════════════════════════════════════════════════════════
Scenario: fan_out
   "List all projects, then summarize the issues in each one."
   Description: 1 list + N detail calls, requires per-project reasoning
   Tools: ["list_projects", "list_issues", "list_issues", "list_issues"]
════════════════════════════════════════════════════════════════════════════════

┌────────────┬──────────┬──────────┬──────────┬────────┬──────────┬──────────┬─────────┐
│   Mode     │ ctx_tok  │ cmp_tok  │ total_tok│  cost  │ latency  │ evidence │ quality │
├────────────┼──────────┼──────────┼──────────┼────────┼──────────┼──────────┼─────────┤
│ baseline   │   137300 │      400 │   137700 │ $0.0208 │  38900ms │        0 │      0% │
│ codemode   │    13900 │      220 │    14120 │ $0.0022 │  12950ms │        0 │     40% │
│ rlm        │      900 │     2700 │     3600 │ $0.0018 │  12947ms │        4 │    100% │
└────────────┴──────────┴──────────┴──────────┴────────┴──────────┴──────────┴─────────┘
```

### Key Indicators

**RLM Advantage**: Look for scenarios where:
- Baseline shows 0% quality (context overflow)
- Code Mode shows <100% quality (lossy summarization)
- RLM maintains 100% quality with lower total tokens

**Trade-off Analysis**:
- RLM uses more completion tokens (sub-LM calls) but far fewer context tokens
- Net token savings typically 74-98% vs baseline
- Quality maintained through bounded decomposition

### Infrastructure Overhead

```
| Content Size | Iterations | Min | Avg | Max |
|--------------|------------|-----|-----|-----|
| 4KB          | 10         | 0.330ms | 0.369ms | 0.400ms |
| 200KB        | 10         | 13.267ms | 13.460ms | 13.589ms |
```

**Acceptable if**: RLM overhead << tool latency (typically 200-13000ms)

## Architecture Reference

```
┌─────────────────────────────────────────────────────────────────┐
│                    User Query                                    │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│              Gateway Tool Execution                              │
│  • MCP over StreamableHttp                                      │
│  • Returns: tool results (potentially large)                    │
└─────────────────────────────────────────────────────────────────┘
                              │
            ┌─────────────────┼─────────────────┐
            ▼                 ▼                 ▼
     ┌──────────┐      ┌──────────┐      ┌──────────┐
     │ Baseline │      │ Code Mode│      │ RLM Mode │
     └──────────┘      └──────────┘      └──────────┘
            │                 │                 │
            ▼                 ▼                 ▼
┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐
│ Full results in  │ │ Summarized       │ │ GatewayRouter    │
│ conversation     │ │ results in       │ │ routes by size:  │
│ context          │ │ context          │ │ • <2K: pass thru │
│                  │ │                  │ │ • >2K: corpus    │
└──────────────────┘ └──────────────────┘ └──────────────────┘
            │                 │                 │
            ▼                 ▼                 ▼
┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐
│ LLM sees all     │ │ LLM sees         │ │ LLM sees evidence│
│ data directly    │ │ summaries only   │ │ summaries, uses  │
│                  │ │ (lossy)          │ │ rlm_search,      │
│                  │ │                  │ │ rlm_get_chunk,   │
│                  │ │                  │ │ sub-LM calls     │
└──────────────────┘ └──────────────────┘ └──────────────────┘
            │                 │                 │
            ▼                 ▼                 ▼
┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐
│ Quality: varies  │ │ Quality: limited │ │ Quality: full    │
│ (overflow risk)  │ │ (no reasoning)   │ │ (bounded decomp) │
│ Provenance: none │ │ Provenance: none │ │ Provenance: DAG  │
└──────────────────┘ └──────────────────┘ └──────────────────┘
```

## Extending Benchmarks

### Adding New Scenarios

```rust
const BENCHMARK_SCENARIOS: &[BenchmarkScenario] = &[
    // Add new scenario
    BenchmarkScenario {
        id: "new_scenario",
        prompt: "Your query here",
        tools: &["tool1", "tool2"],
        description: "What this tests",
    },
];
```

### Adding New Tool Simulations

```rust
const SIMULATED_TOOL_RESULTS: &[SimulatedToolResult] = &[
    // Add new tool
    SimulatedToolResult {
        name: "new_tool",
        tokens: 5000,
        latency_ms: 300,
        description: "What this returns",
    },
];
```

### Adjusting Quality Scores

Update `benchmark_codemode()` quality logic for new scenarios:

```rust
let quality_score = match scenario.id {
    "simple_list" => 100,
    "fan_out" => 40,
    "new_scenario" => 60,  // Your assessment
    _ => 50,
};
```

## References

- RLM Implementation: `core/src/rlm/`
- Gateway Intercept: `core/src/rlm/gateway_intercept.rs`
- Benchmark Tests: `core/tests/rlm_gateway_benchmark.rs`
- Corpus Storage: `core/src/rlm/corpus.rs`
- Evidence Tracking: `core/src/rlm/evidence.rs`
