# RLM Evaluation Methodology

This document describes the evaluation methodology for benchmarking the RLM (Retrieval-augmented Language Model) system, including the mathematical formulas used to measure performance.

---

## Table of Contents

1. [Overview](#overview)
2. [Execution Modes](#execution-modes)
3. [Primary Metrics](#primary-metrics)
4. [Mathematical Formulas](#mathematical-formulas)
5. [Quality Scoring](#quality-scoring)
6. [Claims-Based Evaluation](#claims-based-evaluation)
7. [Benchmark Scenarios](#benchmark-scenarios)
8. [Cost Estimation](#cost-estimation)

---

## Overview

The evaluation compares three approaches for processing tool results from MCP Gateway servers:

| Approach | Description | Token Strategy |
|----------|-------------|----------------|
| **Baseline** | EXECUTE_TOOL - Full results in context | All tokens in conversation |
| **Code Mode** | EXECUTE_CODE - Batched with fixed summarization | Summarized tokens only (10%) |
| **RLM Mode** | Corpus storage with bounded access | Evidence summaries + sub-LM decomposition |

---

## Execution Modes

### Baseline Mode (EXECUTE_TOOL)

Direct tool execution where all results are placed in the LLM context window.

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  Tool Call  │────>│ Full Result │────>│ LLM Context │
└─────────────┘     └─────────────┘     └─────────────┘
```

**Characteristics:**
- Sequential execution
- Full data fidelity
- Context overflow risk at ~128K tokens

### Code Mode (EXECUTE_CODE)

Batched JavaScript execution with fixed summarization.

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  JS Code    │────>│ Summarized  │────>│ LLM Context │
│ (batched)   │     │ Result (10%)│     │             │
└─────────────┘     └─────────────┘     └─────────────┘
```

**Characteristics:**
- Parallel execution
- Lossy summarization
- No reasoning capability on raw data

### RLM Mode (EXECUTE_TOOL + RLM)

Intelligent routing with corpus storage for large results.

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  Tool Call  │────>│ Router      │────>│ <2K tokens? │
└─────────────┘     │ (size check)│     └──────┬──────┘
                    └─────────────┘            │
                           │              ┌────┴────┐
                           │         Yes  │         │  No
                           │              ▼         ▼
                           │     ┌──────────┐  ┌──────────┐
                           │     │Passthrough│  │ Corpus   │
                           │     │to Context │  │ Storage  │
                           │     └──────────┘  └────┬─────┘
                           │                        │
                           │                        ▼
                           │               ┌──────────────┐
                           │               │Evidence ID + │
                           │               │Summary (100t)│
                           │               └──────────────┘
```

**Characteristics:**
- Parallel execution
- Full data preserved in corpus
- Sub-LM access via `rlm_search`, `rlm_get_chunk`
- Evidence DAG for provenance

---

## Primary Metrics

### Token Metrics

| Metric | Symbol | Description |
|--------|--------|-------------|
| Context Tokens | $T_{ctx}$ | Tokens in LLM input context |
| Completion Tokens | $T_{cmp}$ | Tokens generated by LLM |
| Total Tokens | $T_{total}$ | Sum of context + completion |

### Performance Metrics

| Metric | Symbol | Description |
|--------|--------|-------------|
| Latency | $L$ | Total execution time (ms) |
| Round Trips | $R$ | Number of LLM API calls |
| Evidence Count | $E$ | Evidence items tracked (RLM) |
| Chunk Count | $C$ | Corpus chunks created (RLM) |

### Quality Metrics

| Metric | Symbol | Description |
|--------|--------|-------------|
| Quality Score | $Q$ | Reasoning capability preserved (0-100%) |
| Coverage | $Cov$ | Claims fulfilled ratio (0-1.0) |
| Pass/Fail | $P$ | Binary: $Cov \geq 0.75$ |

---

## Mathematical Formulas

### Baseline Mode Calculations

**Context Tokens:**
$$T_{ctx}^{baseline} = T_{system} + \sum_{i=1}^{n} T_{tool_i}$$

Where:
- $T_{system} = 300$ (system prompt overhead)
- $T_{tool_i}$ = tokens from tool result $i$
- $n$ = number of tool calls

**Completion Tokens:**
$$T_{cmp}^{baseline} = 50n + 200$$

Where:
- $50n$ = tokens for tool call requests
- $200$ = final response tokens

**Latency:**
$$L^{baseline} = \sum_{i=1}^{n} L_{tool_i}$$

Sequential execution sums all tool latencies.

**Quality:**
$$Q^{baseline} = \begin{cases} 100 & \text{if } T_{ctx} < 128000 \\ 0 & \text{if } T_{ctx} \geq 128000 \end{cases}$$

### Code Mode Calculations

**Context Tokens:**
$$T_{ctx}^{codemode} = T_{system} + \sum_{i=1}^{n} \frac{T_{tool_i}}{10}$$

Where:
- $T_{system} = 200$ (minimal tool definitions)
- Summarization factor = 0.10 (10% of original)

**Completion Tokens:**
$$T_{cmp}^{codemode} = 100 + 30n$$

Where:
- $100$ = base code generation
- $30n$ = per-tool code overhead

**Latency:**
$$L^{codemode} = \max_{i=1}^{n}(L_{tool_i}) + L_{vm}$$

Where:
- $L_{vm} = 50ms$ (JavaScript VM overhead)
- Parallel execution takes max latency

**Quality:**
$$Q^{codemode} = f(scenario)$$

| Scenario | Quality |
|----------|---------|
| simple_list | 100% |
| fan_out | 40% |
| cross_entity | 25% |
| detail_lookup | 50% |

### RLM Mode Calculations

**Context Tokens:**
$$T_{ctx}^{rlm} = T_{system} + 100 \cdot E$$

Where:
- $T_{system} = 500$ (RLM system prompt + tools)
- $E$ = evidence items created
- $100$ = tokens per evidence summary

**Completion Tokens:**
$$T_{cmp}^{rlm} = 200 + 500 \cdot D$$

Where:
- $200$ = root response tokens
- $D$ = sub-LM decomposition depth
- $500$ = tokens per sub-LM call

**Sub-LM Depth by Scenario:**

| Scenario | Depth ($D$) | Rationale |
|----------|-------------|-----------|
| simple_list | 0 | No decomposition needed |
| fan_out | 5 | Per-project reasoning |
| cross_entity | 11 | Cross-reference analysis |
| detail_lookup | 0 | Direct lookup |

**Latency:**
$$L^{rlm} = \max_{i=1}^{n}(L_{tool_i}) + L_{rlm}$$

Where:
- $L_{rlm}$ = RLM routing overhead (1-15ms)

**Quality:**
$$Q^{rlm} = 100\%$$

RLM maintains full quality through bounded decomposition.

---

## Quality Scoring

### Definition

Quality reflects the approach's ability to perform required reasoning on the data:

| Score | Meaning |
|-------|---------|
| 100% | Full reasoning capability preserved |
| 50% | Partial - some analysis possible |
| 25% | Minimal - only basic aggregation |
| 0% | Failure - context overflow or error |

### Quality Matrix by Scenario

| Scenario | Baseline | Code Mode | RLM |
|----------|----------|-----------|-----|
| simple_list | 100% | 100% | 100% |
| fan_out | **0%*** | 40% | 100% |
| cross_entity | 100% | 25% | 100% |
| detail_lookup | 100% | 50% | 100% |

*Baseline fails on fan_out due to context overflow (137K > 128K limit)

### Quality Degradation Factors

**Baseline:**
- Context overflow ($T_{ctx} > 128K$) → Quality = 0%
- No provenance tracking

**Code Mode:**
- Lossy summarization → Cannot reason on original data
- Fixed aggregation → Cannot perform cross-entity analysis
- No LLM reasoning in code execution

**RLM:**
- Bounded access → May require multiple sub-LM calls
- Overhead for simple queries → Slightly more completion tokens

---

## Claims-Based Evaluation

For MCP-Atlas benchmark tasks, we use claims-based evaluation with GPT-4o as judge.

### Claim Scores

$$S_{claim} = \begin{cases}
1.0 & \text{FULFILLED} \\
0.5 & \text{PARTIALLY\_FULFILLED} \\
0.0 & \text{NOT\_FULFILLED}
\end{cases}$$

### Coverage Calculation

$$Cov = \frac{1}{|C|} \sum_{c \in C} S_c$$

Where:
- $C$ = set of claims
- $S_c$ = score for claim $c$

### Pass Threshold

$$P = \begin{cases}
\text{PASS} & \text{if } Cov \geq 0.75 \\
\text{FAIL} & \text{if } Cov < 0.75
\end{cases}$$

### Judge Prompt Structure

```
System: You are an expert evaluator for AI agent responses...

User:
## Task Prompt
{original_task}

## Agent's Answer
{agent_response}

## Claims to Verify
1. {claim_1}
2. {claim_2}
...

Evaluate each claim and respond with JSON array.
```

### Example Verification

**Claims:**
1. "The response lists all 15 projects"
2. "Each project includes its status"
3. "The total issue count is provided"

**Judge Response:**
```json
[
  {"claim": "...", "verdict": "FULFILLED", "reason": "All 15 projects listed"},
  {"claim": "...", "verdict": "PARTIALLY_FULFILLED", "reason": "Status missing for 2 projects"},
  {"claim": "...", "verdict": "FULFILLED", "reason": "Total of 847 issues stated"}
]
```

**Coverage:**
$$Cov = \frac{1.0 + 0.5 + 1.0}{3} = 0.833$$

**Result:** PASS ($0.833 \geq 0.75$)

---

## Benchmark Scenarios

### Scenario Definitions

| Scenario | Query | Tools | Pattern |
|----------|-------|-------|---------|
| **simple_list** | "Get count of issues and projects" | list_issues, list_projects | 2 parallel calls |
| **fan_out** | "Summarize issues per project" | list_projects, list_issues ×3 | 1 list + N detail |
| **cross_entity** | "Analyze team workload" | list_users, list_projects, list_issues, list_documents | Multi-entity |
| **detail_lookup** | "Get 5 issue details" | get_issue ×5 | Multiple lookups |

### Simulated Tool Response Sizes

| Tool | Tokens | Latency | Description |
|------|--------|---------|-------------|
| list_projects | 2,000 | 200ms | 15 projects with metadata |
| list_issues | 45,000 | 12,900ms | 200 issues with full details |
| list_users | 500 | 87ms | 50 users with basic info |
| get_issue | 800 | 150ms | Single issue with comments |
| list_documents | 8,000 | 235ms | 100 documents with content |

### Token Calculation Example: Fan-Out Scenario

**Baseline:**
$$T_{ctx}^{baseline} = 300 + 2000 + 45000 + 45000 + 45000 = 137,300$$
$$T_{cmp}^{baseline} = 50(4) + 200 = 400$$
$$T_{total}^{baseline} = 137,700$$

**Code Mode:**
$$T_{ctx}^{codemode} = 200 + \frac{2000 + 45000 + 45000 + 45000}{10} = 13,900$$
$$T_{cmp}^{codemode} = 100 + 30(4) = 220$$
$$T_{total}^{codemode} = 14,120$$

**RLM:**
$$T_{ctx}^{rlm} = 500 + 100(4) = 900$$
$$T_{cmp}^{rlm} = 200 + 500(5) = 2,700$$
$$T_{total}^{rlm} = 3,600$$

**Token Reduction vs Baseline:**
- Code Mode: $\frac{14,120 - 137,700}{137,700} = -89.7\%$
- RLM: $\frac{3,600 - 137,700}{137,700} = -97.4\%$

---

## Cost Estimation

### Pricing Model (GPT-4o-mini representative)

| Component | Price |
|-----------|-------|
| Prompt (input) | $0.15 / 1M tokens |
| Completion (output) | $0.60 / 1M tokens |

### Cost Formula

$$Cost = \frac{T_{ctx} \times 0.15 + T_{cmp} \times 0.60}{1,000,000}$$

### Cost Comparison by Scenario

| Scenario | Baseline | Code Mode | RLM |
|----------|----------|-----------|-----|
| simple_list | $0.0073 | $0.0008 | **$0.0002** |
| fan_out | $0.0208 | $0.0022 | **$0.0018** |
| cross_entity | $0.0086 | $0.0010 | $0.0036 |
| detail_lookup | $0.0009 | $0.0002 | $0.0003 |

---

## Infrastructure Overhead

### Corpus Append Performance

Measured latency for appending content to the RLM corpus:

$$L_{append} \approx 0.07 \cdot T_{kb}$$

Where $T_{kb}$ is content size in KB.

| Content Size | Avg Latency |
|--------------|-------------|
| 4KB | 0.4ms |
| 20KB | 1.4ms |
| 40KB | 2.8ms |
| 200KB | 13.5ms |

### Gateway Router Performance

| Result Size | Route | Latency |
|-------------|-------|---------|
| <2K tokens | passthrough | <0.01ms |
| >2K tokens | corpus | ~0.25ms per 1K tokens |

### Overhead Ratio

$$\text{Overhead Ratio} = \frac{L_{rlm}}{L_{tool}}$$

For typical tool calls (200-13,000ms), RLM overhead (1-15ms) represents:
$$\text{Overhead Ratio} < 0.1\%$$

---

## Summary

### When to Use Each Mode

| Scenario | Recommended Mode | Rationale |
|----------|------------------|-----------|
| Small results (<2K tokens) | Baseline | No overhead, full data |
| Speed-critical, lossy OK | Code Mode | Fastest, lowest cost |
| Large results (>2K tokens) | **RLM** | Bounded context, full quality |
| Complex analysis | **RLM** | Sub-LM reasoning preserved |
| Audit requirements | **RLM** | Evidence DAG provenance |

### Key Formulas Reference

| Metric | Baseline | Code Mode | RLM |
|--------|----------|-----------|-----|
| $T_{ctx}$ | $300 + \sum T_{tool}$ | $200 + 0.1\sum T_{tool}$ | $500 + 100E$ |
| $T_{cmp}$ | $50n + 200$ | $100 + 30n$ | $200 + 500D$ |
| $L$ | $\sum L_{tool}$ | $\max(L_{tool}) + 50$ | $\max(L_{tool}) + L_{rlm}$ |
| $Q$ | $100\%$ or $0\%$ | $25-100\%$ | $100\%$ |

---

## References

- RLM Implementation: `core/src/rlm/`
- Benchmark Tests: `core/tests/rlm_gateway_benchmark.rs`
- Integration Tests: `core/tests/rlm_gateway_integration.rs`
- Eval Module: `core/src/eval/`
- Claims Judge: `core/src/eval/judge.rs`
